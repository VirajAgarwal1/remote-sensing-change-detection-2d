{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.swin_transformer import swin_t_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn((1,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model = swin_t_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = enc_model(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 64, 64])\n",
      "torch.Size([1, 192, 32, 32])\n",
      "torch.Size([1, 384, 16, 16])\n",
      "torch.Size([1, 768, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "for out in c:\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.encoder import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder(\"swin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Unfold-1             [10, 48, 4096]               0\n",
      "            Linear-2           [10, 64, 64, 96]           4,704\n",
      "      PatchMerging-3           [10, 64, 64, 96]               0\n",
      "         LayerNorm-4           [10, 64, 64, 96]             192\n",
      "            Linear-5          [10, 64, 64, 288]          27,648\n",
      "            Linear-6           [10, 64, 64, 96]           9,312\n",
      "   WindowAttention-7           [10, 64, 64, 96]               0\n",
      "           PreNorm-8           [10, 64, 64, 96]               0\n",
      "          Residual-9           [10, 64, 64, 96]               0\n",
      "        LayerNorm-10           [10, 64, 64, 96]             192\n",
      "           Linear-11          [10, 64, 64, 384]          37,248\n",
      "             GELU-12          [10, 64, 64, 384]               0\n",
      "           Linear-13           [10, 64, 64, 96]          36,960\n",
      "      FeedForward-14           [10, 64, 64, 96]               0\n",
      "          PreNorm-15           [10, 64, 64, 96]               0\n",
      "         Residual-16           [10, 64, 64, 96]               0\n",
      "        SwinBlock-17           [10, 64, 64, 96]               0\n",
      "        LayerNorm-18           [10, 64, 64, 96]             192\n",
      "      CyclicShift-19           [10, 64, 64, 96]               0\n",
      "           Linear-20          [10, 64, 64, 288]          27,648\n",
      "           Linear-21           [10, 64, 64, 96]           9,312\n",
      "      CyclicShift-22           [10, 64, 64, 96]               0\n",
      "  WindowAttention-23           [10, 64, 64, 96]               0\n",
      "          PreNorm-24           [10, 64, 64, 96]               0\n",
      "         Residual-25           [10, 64, 64, 96]               0\n",
      "        LayerNorm-26           [10, 64, 64, 96]             192\n",
      "           Linear-27          [10, 64, 64, 384]          37,248\n",
      "             GELU-28          [10, 64, 64, 384]               0\n",
      "           Linear-29           [10, 64, 64, 96]          36,960\n",
      "      FeedForward-30           [10, 64, 64, 96]               0\n",
      "          PreNorm-31           [10, 64, 64, 96]               0\n",
      "         Residual-32           [10, 64, 64, 96]               0\n",
      "        SwinBlock-33           [10, 64, 64, 96]               0\n",
      "      StageModule-34           [10, 96, 64, 64]               0\n",
      "           Unfold-35            [10, 384, 1024]               0\n",
      "           Linear-36          [10, 32, 32, 192]          73,920\n",
      "     PatchMerging-37          [10, 32, 32, 192]               0\n",
      "        LayerNorm-38          [10, 32, 32, 192]             384\n",
      "           Linear-39          [10, 32, 32, 576]         110,592\n",
      "           Linear-40          [10, 32, 32, 192]          37,056\n",
      "  WindowAttention-41          [10, 32, 32, 192]               0\n",
      "          PreNorm-42          [10, 32, 32, 192]               0\n",
      "         Residual-43          [10, 32, 32, 192]               0\n",
      "        LayerNorm-44          [10, 32, 32, 192]             384\n",
      "           Linear-45          [10, 32, 32, 768]         148,224\n",
      "             GELU-46          [10, 32, 32, 768]               0\n",
      "           Linear-47          [10, 32, 32, 192]         147,648\n",
      "      FeedForward-48          [10, 32, 32, 192]               0\n",
      "          PreNorm-49          [10, 32, 32, 192]               0\n",
      "         Residual-50          [10, 32, 32, 192]               0\n",
      "        SwinBlock-51          [10, 32, 32, 192]               0\n",
      "        LayerNorm-52          [10, 32, 32, 192]             384\n",
      "      CyclicShift-53          [10, 32, 32, 192]               0\n",
      "           Linear-54          [10, 32, 32, 576]         110,592\n",
      "           Linear-55          [10, 32, 32, 192]          37,056\n",
      "      CyclicShift-56          [10, 32, 32, 192]               0\n",
      "  WindowAttention-57          [10, 32, 32, 192]               0\n",
      "          PreNorm-58          [10, 32, 32, 192]               0\n",
      "         Residual-59          [10, 32, 32, 192]               0\n",
      "        LayerNorm-60          [10, 32, 32, 192]             384\n",
      "           Linear-61          [10, 32, 32, 768]         148,224\n",
      "             GELU-62          [10, 32, 32, 768]               0\n",
      "           Linear-63          [10, 32, 32, 192]         147,648\n",
      "      FeedForward-64          [10, 32, 32, 192]               0\n",
      "          PreNorm-65          [10, 32, 32, 192]               0\n",
      "         Residual-66          [10, 32, 32, 192]               0\n",
      "        SwinBlock-67          [10, 32, 32, 192]               0\n",
      "      StageModule-68          [10, 192, 32, 32]               0\n",
      "           Unfold-69             [10, 768, 256]               0\n",
      "           Linear-70          [10, 16, 16, 384]         295,296\n",
      "     PatchMerging-71          [10, 16, 16, 384]               0\n",
      "        LayerNorm-72          [10, 16, 16, 384]             768\n",
      "           Linear-73         [10, 16, 16, 1152]         442,368\n",
      "           Linear-74          [10, 16, 16, 384]         147,840\n",
      "  WindowAttention-75          [10, 16, 16, 384]               0\n",
      "          PreNorm-76          [10, 16, 16, 384]               0\n",
      "         Residual-77          [10, 16, 16, 384]               0\n",
      "        LayerNorm-78          [10, 16, 16, 384]             768\n",
      "           Linear-79         [10, 16, 16, 1536]         591,360\n",
      "             GELU-80         [10, 16, 16, 1536]               0\n",
      "           Linear-81          [10, 16, 16, 384]         590,208\n",
      "      FeedForward-82          [10, 16, 16, 384]               0\n",
      "          PreNorm-83          [10, 16, 16, 384]               0\n",
      "         Residual-84          [10, 16, 16, 384]               0\n",
      "        SwinBlock-85          [10, 16, 16, 384]               0\n",
      "        LayerNorm-86          [10, 16, 16, 384]             768\n",
      "      CyclicShift-87          [10, 16, 16, 384]               0\n",
      "           Linear-88         [10, 16, 16, 1152]         442,368\n",
      "           Linear-89          [10, 16, 16, 384]         147,840\n",
      "      CyclicShift-90          [10, 16, 16, 384]               0\n",
      "  WindowAttention-91          [10, 16, 16, 384]               0\n",
      "          PreNorm-92          [10, 16, 16, 384]               0\n",
      "         Residual-93          [10, 16, 16, 384]               0\n",
      "        LayerNorm-94          [10, 16, 16, 384]             768\n",
      "           Linear-95         [10, 16, 16, 1536]         591,360\n",
      "             GELU-96         [10, 16, 16, 1536]               0\n",
      "           Linear-97          [10, 16, 16, 384]         590,208\n",
      "      FeedForward-98          [10, 16, 16, 384]               0\n",
      "          PreNorm-99          [10, 16, 16, 384]               0\n",
      "        Residual-100          [10, 16, 16, 384]               0\n",
      "       SwinBlock-101          [10, 16, 16, 384]               0\n",
      "       LayerNorm-102          [10, 16, 16, 384]             768\n",
      "          Linear-103         [10, 16, 16, 1152]         442,368\n",
      "          Linear-104          [10, 16, 16, 384]         147,840\n",
      " WindowAttention-105          [10, 16, 16, 384]               0\n",
      "         PreNorm-106          [10, 16, 16, 384]               0\n",
      "        Residual-107          [10, 16, 16, 384]               0\n",
      "       LayerNorm-108          [10, 16, 16, 384]             768\n",
      "          Linear-109         [10, 16, 16, 1536]         591,360\n",
      "            GELU-110         [10, 16, 16, 1536]               0\n",
      "          Linear-111          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-112          [10, 16, 16, 384]               0\n",
      "         PreNorm-113          [10, 16, 16, 384]               0\n",
      "        Residual-114          [10, 16, 16, 384]               0\n",
      "       SwinBlock-115          [10, 16, 16, 384]               0\n",
      "       LayerNorm-116          [10, 16, 16, 384]             768\n",
      "     CyclicShift-117          [10, 16, 16, 384]               0\n",
      "          Linear-118         [10, 16, 16, 1152]         442,368\n",
      "          Linear-119          [10, 16, 16, 384]         147,840\n",
      "     CyclicShift-120          [10, 16, 16, 384]               0\n",
      " WindowAttention-121          [10, 16, 16, 384]               0\n",
      "         PreNorm-122          [10, 16, 16, 384]               0\n",
      "        Residual-123          [10, 16, 16, 384]               0\n",
      "       LayerNorm-124          [10, 16, 16, 384]             768\n",
      "          Linear-125         [10, 16, 16, 1536]         591,360\n",
      "            GELU-126         [10, 16, 16, 1536]               0\n",
      "          Linear-127          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-128          [10, 16, 16, 384]               0\n",
      "         PreNorm-129          [10, 16, 16, 384]               0\n",
      "        Residual-130          [10, 16, 16, 384]               0\n",
      "       SwinBlock-131          [10, 16, 16, 384]               0\n",
      "       LayerNorm-132          [10, 16, 16, 384]             768\n",
      "          Linear-133         [10, 16, 16, 1152]         442,368\n",
      "          Linear-134          [10, 16, 16, 384]         147,840\n",
      " WindowAttention-135          [10, 16, 16, 384]               0\n",
      "         PreNorm-136          [10, 16, 16, 384]               0\n",
      "        Residual-137          [10, 16, 16, 384]               0\n",
      "       LayerNorm-138          [10, 16, 16, 384]             768\n",
      "          Linear-139         [10, 16, 16, 1536]         591,360\n",
      "            GELU-140         [10, 16, 16, 1536]               0\n",
      "          Linear-141          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-142          [10, 16, 16, 384]               0\n",
      "         PreNorm-143          [10, 16, 16, 384]               0\n",
      "        Residual-144          [10, 16, 16, 384]               0\n",
      "       SwinBlock-145          [10, 16, 16, 384]               0\n",
      "       LayerNorm-146          [10, 16, 16, 384]             768\n",
      "     CyclicShift-147          [10, 16, 16, 384]               0\n",
      "          Linear-148         [10, 16, 16, 1152]         442,368\n",
      "          Linear-149          [10, 16, 16, 384]         147,840\n",
      "     CyclicShift-150          [10, 16, 16, 384]               0\n",
      " WindowAttention-151          [10, 16, 16, 384]               0\n",
      "         PreNorm-152          [10, 16, 16, 384]               0\n",
      "        Residual-153          [10, 16, 16, 384]               0\n",
      "       LayerNorm-154          [10, 16, 16, 384]             768\n",
      "          Linear-155         [10, 16, 16, 1536]         591,360\n",
      "            GELU-156         [10, 16, 16, 1536]               0\n",
      "          Linear-157          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-158          [10, 16, 16, 384]               0\n",
      "         PreNorm-159          [10, 16, 16, 384]               0\n",
      "        Residual-160          [10, 16, 16, 384]               0\n",
      "       SwinBlock-161          [10, 16, 16, 384]               0\n",
      "     StageModule-162          [10, 384, 16, 16]               0\n",
      "          Unfold-163             [10, 1536, 64]               0\n",
      "          Linear-164            [10, 8, 8, 768]       1,180,416\n",
      "    PatchMerging-165            [10, 8, 8, 768]               0\n",
      "       LayerNorm-166            [10, 8, 8, 768]           1,536\n",
      "          Linear-167           [10, 8, 8, 2304]       1,769,472\n",
      "          Linear-168            [10, 8, 8, 768]         590,592\n",
      " WindowAttention-169            [10, 8, 8, 768]               0\n",
      "         PreNorm-170            [10, 8, 8, 768]               0\n",
      "        Residual-171            [10, 8, 8, 768]               0\n",
      "       LayerNorm-172            [10, 8, 8, 768]           1,536\n",
      "          Linear-173           [10, 8, 8, 3072]       2,362,368\n",
      "            GELU-174           [10, 8, 8, 3072]               0\n",
      "          Linear-175            [10, 8, 8, 768]       2,360,064\n",
      "     FeedForward-176            [10, 8, 8, 768]               0\n",
      "         PreNorm-177            [10, 8, 8, 768]               0\n",
      "        Residual-178            [10, 8, 8, 768]               0\n",
      "       SwinBlock-179            [10, 8, 8, 768]               0\n",
      "       LayerNorm-180            [10, 8, 8, 768]           1,536\n",
      "     CyclicShift-181            [10, 8, 8, 768]               0\n",
      "          Linear-182           [10, 8, 8, 2304]       1,769,472\n",
      "          Linear-183            [10, 8, 8, 768]         590,592\n",
      "     CyclicShift-184            [10, 8, 8, 768]               0\n",
      " WindowAttention-185            [10, 8, 8, 768]               0\n",
      "         PreNorm-186            [10, 8, 8, 768]               0\n",
      "        Residual-187            [10, 8, 8, 768]               0\n",
      "       LayerNorm-188            [10, 8, 8, 768]           1,536\n",
      "          Linear-189           [10, 8, 8, 3072]       2,362,368\n",
      "            GELU-190           [10, 8, 8, 3072]               0\n",
      "          Linear-191            [10, 8, 8, 768]       2,360,064\n",
      "     FeedForward-192            [10, 8, 8, 768]               0\n",
      "         PreNorm-193            [10, 8, 8, 768]               0\n",
      "        Residual-194            [10, 8, 8, 768]               0\n",
      "       SwinBlock-195            [10, 8, 8, 768]               0\n",
      "     StageModule-196            [10, 768, 8, 8]               0\n",
      " SwinTransformer-197  [[-1, 96, 64, 64], [-1, 192, 32, 32], [-1, 384, 16, 16], [-1, 768, 8, 8]]               0\n",
      "          Unfold-198             [10, 48, 4096]               0\n",
      "          Linear-199           [10, 64, 64, 96]           4,704\n",
      "    PatchMerging-200           [10, 64, 64, 96]               0\n",
      "       LayerNorm-201           [10, 64, 64, 96]             192\n",
      "          Linear-202          [10, 64, 64, 288]          27,648\n",
      "          Linear-203           [10, 64, 64, 96]           9,312\n",
      " WindowAttention-204           [10, 64, 64, 96]               0\n",
      "         PreNorm-205           [10, 64, 64, 96]               0\n",
      "        Residual-206           [10, 64, 64, 96]               0\n",
      "       LayerNorm-207           [10, 64, 64, 96]             192\n",
      "          Linear-208          [10, 64, 64, 384]          37,248\n",
      "            GELU-209          [10, 64, 64, 384]               0\n",
      "          Linear-210           [10, 64, 64, 96]          36,960\n",
      "     FeedForward-211           [10, 64, 64, 96]               0\n",
      "         PreNorm-212           [10, 64, 64, 96]               0\n",
      "        Residual-213           [10, 64, 64, 96]               0\n",
      "       SwinBlock-214           [10, 64, 64, 96]               0\n",
      "       LayerNorm-215           [10, 64, 64, 96]             192\n",
      "     CyclicShift-216           [10, 64, 64, 96]               0\n",
      "          Linear-217          [10, 64, 64, 288]          27,648\n",
      "          Linear-218           [10, 64, 64, 96]           9,312\n",
      "     CyclicShift-219           [10, 64, 64, 96]               0\n",
      " WindowAttention-220           [10, 64, 64, 96]               0\n",
      "         PreNorm-221           [10, 64, 64, 96]               0\n",
      "        Residual-222           [10, 64, 64, 96]               0\n",
      "       LayerNorm-223           [10, 64, 64, 96]             192\n",
      "          Linear-224          [10, 64, 64, 384]          37,248\n",
      "            GELU-225          [10, 64, 64, 384]               0\n",
      "          Linear-226           [10, 64, 64, 96]          36,960\n",
      "     FeedForward-227           [10, 64, 64, 96]               0\n",
      "         PreNorm-228           [10, 64, 64, 96]               0\n",
      "        Residual-229           [10, 64, 64, 96]               0\n",
      "       SwinBlock-230           [10, 64, 64, 96]               0\n",
      "     StageModule-231           [10, 96, 64, 64]               0\n",
      "          Unfold-232            [10, 384, 1024]               0\n",
      "          Linear-233          [10, 32, 32, 192]          73,920\n",
      "    PatchMerging-234          [10, 32, 32, 192]               0\n",
      "       LayerNorm-235          [10, 32, 32, 192]             384\n",
      "          Linear-236          [10, 32, 32, 576]         110,592\n",
      "          Linear-237          [10, 32, 32, 192]          37,056\n",
      " WindowAttention-238          [10, 32, 32, 192]               0\n",
      "         PreNorm-239          [10, 32, 32, 192]               0\n",
      "        Residual-240          [10, 32, 32, 192]               0\n",
      "       LayerNorm-241          [10, 32, 32, 192]             384\n",
      "          Linear-242          [10, 32, 32, 768]         148,224\n",
      "            GELU-243          [10, 32, 32, 768]               0\n",
      "          Linear-244          [10, 32, 32, 192]         147,648\n",
      "     FeedForward-245          [10, 32, 32, 192]               0\n",
      "         PreNorm-246          [10, 32, 32, 192]               0\n",
      "        Residual-247          [10, 32, 32, 192]               0\n",
      "       SwinBlock-248          [10, 32, 32, 192]               0\n",
      "       LayerNorm-249          [10, 32, 32, 192]             384\n",
      "     CyclicShift-250          [10, 32, 32, 192]               0\n",
      "          Linear-251          [10, 32, 32, 576]         110,592\n",
      "          Linear-252          [10, 32, 32, 192]          37,056\n",
      "     CyclicShift-253          [10, 32, 32, 192]               0\n",
      " WindowAttention-254          [10, 32, 32, 192]               0\n",
      "         PreNorm-255          [10, 32, 32, 192]               0\n",
      "        Residual-256          [10, 32, 32, 192]               0\n",
      "       LayerNorm-257          [10, 32, 32, 192]             384\n",
      "          Linear-258          [10, 32, 32, 768]         148,224\n",
      "            GELU-259          [10, 32, 32, 768]               0\n",
      "          Linear-260          [10, 32, 32, 192]         147,648\n",
      "     FeedForward-261          [10, 32, 32, 192]               0\n",
      "         PreNorm-262          [10, 32, 32, 192]               0\n",
      "        Residual-263          [10, 32, 32, 192]               0\n",
      "       SwinBlock-264          [10, 32, 32, 192]               0\n",
      "     StageModule-265          [10, 192, 32, 32]               0\n",
      "          Unfold-266             [10, 768, 256]               0\n",
      "          Linear-267          [10, 16, 16, 384]         295,296\n",
      "    PatchMerging-268          [10, 16, 16, 384]               0\n",
      "       LayerNorm-269          [10, 16, 16, 384]             768\n",
      "          Linear-270         [10, 16, 16, 1152]         442,368\n",
      "          Linear-271          [10, 16, 16, 384]         147,840\n",
      " WindowAttention-272          [10, 16, 16, 384]               0\n",
      "         PreNorm-273          [10, 16, 16, 384]               0\n",
      "        Residual-274          [10, 16, 16, 384]               0\n",
      "       LayerNorm-275          [10, 16, 16, 384]             768\n",
      "          Linear-276         [10, 16, 16, 1536]         591,360\n",
      "            GELU-277         [10, 16, 16, 1536]               0\n",
      "          Linear-278          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-279          [10, 16, 16, 384]               0\n",
      "         PreNorm-280          [10, 16, 16, 384]               0\n",
      "        Residual-281          [10, 16, 16, 384]               0\n",
      "       SwinBlock-282          [10, 16, 16, 384]               0\n",
      "       LayerNorm-283          [10, 16, 16, 384]             768\n",
      "     CyclicShift-284          [10, 16, 16, 384]               0\n",
      "          Linear-285         [10, 16, 16, 1152]         442,368\n",
      "          Linear-286          [10, 16, 16, 384]         147,840\n",
      "     CyclicShift-287          [10, 16, 16, 384]               0\n",
      " WindowAttention-288          [10, 16, 16, 384]               0\n",
      "         PreNorm-289          [10, 16, 16, 384]               0\n",
      "        Residual-290          [10, 16, 16, 384]               0\n",
      "       LayerNorm-291          [10, 16, 16, 384]             768\n",
      "          Linear-292         [10, 16, 16, 1536]         591,360\n",
      "            GELU-293         [10, 16, 16, 1536]               0\n",
      "          Linear-294          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-295          [10, 16, 16, 384]               0\n",
      "         PreNorm-296          [10, 16, 16, 384]               0\n",
      "        Residual-297          [10, 16, 16, 384]               0\n",
      "       SwinBlock-298          [10, 16, 16, 384]               0\n",
      "       LayerNorm-299          [10, 16, 16, 384]             768\n",
      "          Linear-300         [10, 16, 16, 1152]         442,368\n",
      "          Linear-301          [10, 16, 16, 384]         147,840\n",
      " WindowAttention-302          [10, 16, 16, 384]               0\n",
      "         PreNorm-303          [10, 16, 16, 384]               0\n",
      "        Residual-304          [10, 16, 16, 384]               0\n",
      "       LayerNorm-305          [10, 16, 16, 384]             768\n",
      "          Linear-306         [10, 16, 16, 1536]         591,360\n",
      "            GELU-307         [10, 16, 16, 1536]               0\n",
      "          Linear-308          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-309          [10, 16, 16, 384]               0\n",
      "         PreNorm-310          [10, 16, 16, 384]               0\n",
      "        Residual-311          [10, 16, 16, 384]               0\n",
      "       SwinBlock-312          [10, 16, 16, 384]               0\n",
      "       LayerNorm-313          [10, 16, 16, 384]             768\n",
      "     CyclicShift-314          [10, 16, 16, 384]               0\n",
      "          Linear-315         [10, 16, 16, 1152]         442,368\n",
      "          Linear-316          [10, 16, 16, 384]         147,840\n",
      "     CyclicShift-317          [10, 16, 16, 384]               0\n",
      " WindowAttention-318          [10, 16, 16, 384]               0\n",
      "         PreNorm-319          [10, 16, 16, 384]               0\n",
      "        Residual-320          [10, 16, 16, 384]               0\n",
      "       LayerNorm-321          [10, 16, 16, 384]             768\n",
      "          Linear-322         [10, 16, 16, 1536]         591,360\n",
      "            GELU-323         [10, 16, 16, 1536]               0\n",
      "          Linear-324          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-325          [10, 16, 16, 384]               0\n",
      "         PreNorm-326          [10, 16, 16, 384]               0\n",
      "        Residual-327          [10, 16, 16, 384]               0\n",
      "       SwinBlock-328          [10, 16, 16, 384]               0\n",
      "       LayerNorm-329          [10, 16, 16, 384]             768\n",
      "          Linear-330         [10, 16, 16, 1152]         442,368\n",
      "          Linear-331          [10, 16, 16, 384]         147,840\n",
      " WindowAttention-332          [10, 16, 16, 384]               0\n",
      "         PreNorm-333          [10, 16, 16, 384]               0\n",
      "        Residual-334          [10, 16, 16, 384]               0\n",
      "       LayerNorm-335          [10, 16, 16, 384]             768\n",
      "          Linear-336         [10, 16, 16, 1536]         591,360\n",
      "            GELU-337         [10, 16, 16, 1536]               0\n",
      "          Linear-338          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-339          [10, 16, 16, 384]               0\n",
      "         PreNorm-340          [10, 16, 16, 384]               0\n",
      "        Residual-341          [10, 16, 16, 384]               0\n",
      "       SwinBlock-342          [10, 16, 16, 384]               0\n",
      "       LayerNorm-343          [10, 16, 16, 384]             768\n",
      "     CyclicShift-344          [10, 16, 16, 384]               0\n",
      "          Linear-345         [10, 16, 16, 1152]         442,368\n",
      "          Linear-346          [10, 16, 16, 384]         147,840\n",
      "     CyclicShift-347          [10, 16, 16, 384]               0\n",
      " WindowAttention-348          [10, 16, 16, 384]               0\n",
      "         PreNorm-349          [10, 16, 16, 384]               0\n",
      "        Residual-350          [10, 16, 16, 384]               0\n",
      "       LayerNorm-351          [10, 16, 16, 384]             768\n",
      "          Linear-352         [10, 16, 16, 1536]         591,360\n",
      "            GELU-353         [10, 16, 16, 1536]               0\n",
      "          Linear-354          [10, 16, 16, 384]         590,208\n",
      "     FeedForward-355          [10, 16, 16, 384]               0\n",
      "         PreNorm-356          [10, 16, 16, 384]               0\n",
      "        Residual-357          [10, 16, 16, 384]               0\n",
      "       SwinBlock-358          [10, 16, 16, 384]               0\n",
      "     StageModule-359          [10, 384, 16, 16]               0\n",
      "          Unfold-360             [10, 1536, 64]               0\n",
      "          Linear-361            [10, 8, 8, 768]       1,180,416\n",
      "    PatchMerging-362            [10, 8, 8, 768]               0\n",
      "       LayerNorm-363            [10, 8, 8, 768]           1,536\n",
      "          Linear-364           [10, 8, 8, 2304]       1,769,472\n",
      "          Linear-365            [10, 8, 8, 768]         590,592\n",
      " WindowAttention-366            [10, 8, 8, 768]               0\n",
      "         PreNorm-367            [10, 8, 8, 768]               0\n",
      "        Residual-368            [10, 8, 8, 768]               0\n",
      "       LayerNorm-369            [10, 8, 8, 768]           1,536\n",
      "          Linear-370           [10, 8, 8, 3072]       2,362,368\n",
      "            GELU-371           [10, 8, 8, 3072]               0\n",
      "          Linear-372            [10, 8, 8, 768]       2,360,064\n",
      "     FeedForward-373            [10, 8, 8, 768]               0\n",
      "         PreNorm-374            [10, 8, 8, 768]               0\n",
      "        Residual-375            [10, 8, 8, 768]               0\n",
      "       SwinBlock-376            [10, 8, 8, 768]               0\n",
      "       LayerNorm-377            [10, 8, 8, 768]           1,536\n",
      "     CyclicShift-378            [10, 8, 8, 768]               0\n",
      "          Linear-379           [10, 8, 8, 2304]       1,769,472\n",
      "          Linear-380            [10, 8, 8, 768]         590,592\n",
      "     CyclicShift-381            [10, 8, 8, 768]               0\n",
      " WindowAttention-382            [10, 8, 8, 768]               0\n",
      "         PreNorm-383            [10, 8, 8, 768]               0\n",
      "        Residual-384            [10, 8, 8, 768]               0\n",
      "       LayerNorm-385            [10, 8, 8, 768]           1,536\n",
      "          Linear-386           [10, 8, 8, 3072]       2,362,368\n",
      "            GELU-387           [10, 8, 8, 3072]               0\n",
      "          Linear-388            [10, 8, 8, 768]       2,360,064\n",
      "     FeedForward-389            [10, 8, 8, 768]               0\n",
      "         PreNorm-390            [10, 8, 8, 768]               0\n",
      "        Residual-391            [10, 8, 8, 768]               0\n",
      "       SwinBlock-392            [10, 8, 8, 768]               0\n",
      "     StageModule-393            [10, 768, 8, 8]               0\n",
      " SwinTransformer-394  [[-1, 96, 64, 64], [-1, 192, 32, 32], [-1, 384, 16, 16], [-1, 768, 8, 8]]               0\n",
      "================================================================\n",
      "Total params: 54,954,048\n",
      "Trainable params: 54,954,048\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1474560.00\n",
      "Forward/backward pass size (MB): 70368744170636.50\n",
      "Params size (MB): 209.63\n",
      "Estimated Total Size (MB): 70368745645406.12\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spaaceship/.local/lib/python3.10/site-packages/torchsummary/torchsummary.py:93: RuntimeWarning: overflow encountered in scalar add\n",
      "  total_output += np.prod(summary[layer][\"output_shape\"])\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size = [(3,256,256), (3,256,256)], batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+----------------+\n",
      "|                             Module                             | Requires grad? |\n",
      "+----------------------------------------------------------------+----------------+\n",
      "|           model.stage1.patch_partition.linear.weight           |      True      |\n",
      "|            model.stage1.patch_partition.linear.bias            |      True      |\n",
      "|     model.stage1.layers.0.0.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage1.layers.0.0.attention_block.fn.norm.bias      |      True      |\n",
      "|  model.stage1.layers.0.0.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage1.layers.0.0.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage1.layers.0.0.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage1.layers.0.0.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage1.layers.0.0.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage1.layers.0.0.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage1.layers.0.0.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage1.layers.0.0.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage1.layers.0.0.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage1.layers.0.0.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|     model.stage1.layers.0.1.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage1.layers.0.1.attention_block.fn.norm.bias      |      True      |\n",
      "| model.stage1.layers.0.1.attention_block.fn.fn.upper_lower_mask |      True      |\n",
      "| model.stage1.layers.0.1.attention_block.fn.fn.left_right_mask  |      True      |\n",
      "|  model.stage1.layers.0.1.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage1.layers.0.1.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage1.layers.0.1.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage1.layers.0.1.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage1.layers.0.1.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage1.layers.0.1.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage1.layers.0.1.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage1.layers.0.1.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage1.layers.0.1.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage1.layers.0.1.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|           model.stage2.patch_partition.linear.weight           |      True      |\n",
      "|            model.stage2.patch_partition.linear.bias            |      True      |\n",
      "|     model.stage2.layers.0.0.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage2.layers.0.0.attention_block.fn.norm.bias      |      True      |\n",
      "|  model.stage2.layers.0.0.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage2.layers.0.0.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage2.layers.0.0.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage2.layers.0.0.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage2.layers.0.0.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage2.layers.0.0.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage2.layers.0.0.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage2.layers.0.0.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage2.layers.0.0.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage2.layers.0.0.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|     model.stage2.layers.0.1.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage2.layers.0.1.attention_block.fn.norm.bias      |      True      |\n",
      "| model.stage2.layers.0.1.attention_block.fn.fn.upper_lower_mask |      True      |\n",
      "| model.stage2.layers.0.1.attention_block.fn.fn.left_right_mask  |      True      |\n",
      "|  model.stage2.layers.0.1.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage2.layers.0.1.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage2.layers.0.1.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage2.layers.0.1.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage2.layers.0.1.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage2.layers.0.1.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage2.layers.0.1.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage2.layers.0.1.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage2.layers.0.1.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage2.layers.0.1.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|           model.stage3.patch_partition.linear.weight           |      True      |\n",
      "|            model.stage3.patch_partition.linear.bias            |      True      |\n",
      "|     model.stage3.layers.0.0.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage3.layers.0.0.attention_block.fn.norm.bias      |      True      |\n",
      "|  model.stage3.layers.0.0.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage3.layers.0.0.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage3.layers.0.0.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage3.layers.0.0.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage3.layers.0.0.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage3.layers.0.0.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage3.layers.0.0.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage3.layers.0.0.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage3.layers.0.0.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage3.layers.0.0.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|     model.stage3.layers.0.1.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage3.layers.0.1.attention_block.fn.norm.bias      |      True      |\n",
      "| model.stage3.layers.0.1.attention_block.fn.fn.upper_lower_mask |      True      |\n",
      "| model.stage3.layers.0.1.attention_block.fn.fn.left_right_mask  |      True      |\n",
      "|  model.stage3.layers.0.1.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage3.layers.0.1.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage3.layers.0.1.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage3.layers.0.1.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage3.layers.0.1.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage3.layers.0.1.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage3.layers.0.1.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage3.layers.0.1.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage3.layers.0.1.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage3.layers.0.1.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|     model.stage3.layers.1.0.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage3.layers.1.0.attention_block.fn.norm.bias      |      True      |\n",
      "|  model.stage3.layers.1.0.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage3.layers.1.0.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage3.layers.1.0.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage3.layers.1.0.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage3.layers.1.0.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage3.layers.1.0.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage3.layers.1.0.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage3.layers.1.0.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage3.layers.1.0.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage3.layers.1.0.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|     model.stage3.layers.1.1.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage3.layers.1.1.attention_block.fn.norm.bias      |      True      |\n",
      "| model.stage3.layers.1.1.attention_block.fn.fn.upper_lower_mask |      True      |\n",
      "| model.stage3.layers.1.1.attention_block.fn.fn.left_right_mask  |      True      |\n",
      "|  model.stage3.layers.1.1.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage3.layers.1.1.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage3.layers.1.1.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage3.layers.1.1.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage3.layers.1.1.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage3.layers.1.1.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage3.layers.1.1.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage3.layers.1.1.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage3.layers.1.1.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage3.layers.1.1.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|     model.stage3.layers.2.0.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage3.layers.2.0.attention_block.fn.norm.bias      |      True      |\n",
      "|  model.stage3.layers.2.0.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage3.layers.2.0.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage3.layers.2.0.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage3.layers.2.0.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage3.layers.2.0.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage3.layers.2.0.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage3.layers.2.0.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage3.layers.2.0.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage3.layers.2.0.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage3.layers.2.0.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|     model.stage3.layers.2.1.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage3.layers.2.1.attention_block.fn.norm.bias      |      True      |\n",
      "| model.stage3.layers.2.1.attention_block.fn.fn.upper_lower_mask |      True      |\n",
      "| model.stage3.layers.2.1.attention_block.fn.fn.left_right_mask  |      True      |\n",
      "|  model.stage3.layers.2.1.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage3.layers.2.1.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage3.layers.2.1.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage3.layers.2.1.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage3.layers.2.1.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage3.layers.2.1.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage3.layers.2.1.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage3.layers.2.1.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage3.layers.2.1.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage3.layers.2.1.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|           model.stage4.patch_partition.linear.weight           |      True      |\n",
      "|            model.stage4.patch_partition.linear.bias            |      True      |\n",
      "|     model.stage4.layers.0.0.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage4.layers.0.0.attention_block.fn.norm.bias      |      True      |\n",
      "|  model.stage4.layers.0.0.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage4.layers.0.0.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage4.layers.0.0.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage4.layers.0.0.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage4.layers.0.0.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage4.layers.0.0.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage4.layers.0.0.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage4.layers.0.0.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage4.layers.0.0.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage4.layers.0.0.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "|     model.stage4.layers.0.1.attention_block.fn.norm.weight     |      True      |\n",
      "|      model.stage4.layers.0.1.attention_block.fn.norm.bias      |      True      |\n",
      "| model.stage4.layers.0.1.attention_block.fn.fn.upper_lower_mask |      True      |\n",
      "| model.stage4.layers.0.1.attention_block.fn.fn.left_right_mask  |      True      |\n",
      "|  model.stage4.layers.0.1.attention_block.fn.fn.pos_embedding   |      True      |\n",
      "|  model.stage4.layers.0.1.attention_block.fn.fn.to_qkv.weight   |      True      |\n",
      "|  model.stage4.layers.0.1.attention_block.fn.fn.to_out.weight   |      True      |\n",
      "|   model.stage4.layers.0.1.attention_block.fn.fn.to_out.bias    |      True      |\n",
      "|        model.stage4.layers.0.1.mlp_block.fn.norm.weight        |      True      |\n",
      "|         model.stage4.layers.0.1.mlp_block.fn.norm.bias         |      True      |\n",
      "|      model.stage4.layers.0.1.mlp_block.fn.fn.net.0.weight      |      True      |\n",
      "|       model.stage4.layers.0.1.mlp_block.fn.fn.net.0.bias       |      True      |\n",
      "|      model.stage4.layers.0.1.mlp_block.fn.fn.net.2.weight      |      True      |\n",
      "|       model.stage4.layers.0.1.mlp_block.fn.fn.net.2.bias       |      True      |\n",
      "+----------------------------------------------------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable([\"Module\", \"Requires grad?\"])\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    \n",
    "    # if not parameter.requires_grad: continue\n",
    "    params = parameter.requires_grad\n",
    "    table.add_row([name, params])\n",
    "    \n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everythig clear from the model side... Its working and all the parameters are appropriately set to require gradiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_A.shape =  torch.Size([10, 3, 256, 256])\n",
      "img_B.shape =  torch.Size([10, 3, 256, 256])\n",
      "label.shape =  torch.Size([10])\n",
      "img_A.requires_grad =  True\n",
      "img_B.requires_grad =  True\n",
      "label.requires_grad =  True\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, sample in enumerate(tdl):\n",
    "    img_A = sample[\"img_A\"].clone()\n",
    "    img_B = sample[\"img_B\"].clone()\n",
    "    label = torch.flatten(sample[\"label\"].clone())\n",
    "\n",
    "    print(\"img_A.shape = \", img_A.shape)\n",
    "    print(\"img_B.shape = \", img_B.shape)\n",
    "    print(\"label.shape = \", label.shape)\n",
    "\n",
    "    print(\"img_A.requires_grad = \", img_A.requires_grad)\n",
    "    print(\"img_B.requires_grad = \", img_B.requires_grad)\n",
    "    print(\"label.requires_grad = \", label.requires_grad)\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so the require gradiants are good for the inputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<MeanBackward0 object at 0x7fcca75ed750>\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for batch_idx, sample in enumerate(tdl):\n",
    "\n",
    "    # Move input tensors to the device\n",
    "    img_A = sample[\"img_A\"].clone()\n",
    "    img_B = sample[\"img_B\"].clone()\n",
    "    label = torch.flatten(sample[\"label\"].clone())\n",
    "\n",
    "    # find the loss and update the model parameters accordingly\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    enc1, enc2 = model(img_A, img_B)\n",
    "    # calculate the batch loss\n",
    "\n",
    "    loss = criterion(enc1, enc2, label)\n",
    "    print(type(loss))\n",
    "    print(loss.grad_fn)\n",
    "    # print(loss.grad)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
